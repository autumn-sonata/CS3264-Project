{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Experimental features analysis and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import numpy as np\n",
    "\n",
    "X = pd.read_csv(\"datasets/feature_updated_dataset_X.csv\")\n",
    "y = pd.read_csv(\"datasets/feature_updated_dataset_y.csv\")\n",
    "\n",
    "#rus = RandomOverSampler(random_state=69)\n",
    "#X_rus, y_rus = rus.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "y_train, y_test = y_train.values.ravel(), y_test.values.ravel()\n",
    "# print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_feature1_train = X_train[['http', 'https', 'www', 'url_length', 'url_entropy',  'domain_extension']]\n",
    "# X_feature1_test = X_test[['http', 'https', 'www', 'url_length', 'url_entropy', 'domain_extension']]\n",
    "\n",
    "# X_feature2_train = X_train[['digit_count', 'percentage_count', 'dot_count', 'bs_count', 'dash_count', 'params_count', 'subdomains_count']]\n",
    "# X_feature2_test = X_test[['digit_count', 'percentage_count', 'dot_count', 'bs_count', 'dash_count', 'params_count', 'subdomains_count']]\n",
    "\n",
    "# X_features_boot_train = X_train[['http', 'https', 'www', 'url_length', 'url_entropy',  'domain_extension', 'digit_count', 'percentage_count', 'dot_count', 'dash_count', 'subdomains_count']]\n",
    "# X_features_boot_test = X_test[['http', 'https', 'www', 'url_length', 'url_entropy',  'domain_extension', 'digit_count', 'percentage_count', 'dot_count', 'dash_count', 'subdomains_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "# 1) Logistic Regression\n",
    "model_lr1 = LogisticRegression(max_iter=500)\n",
    "# 2) SVM\n",
    "#model_svm = svm.SVC()\n",
    "# 3) Random Forest Classifier\n",
    "model_rfc1 = RandomForestClassifier(verbose=0, n_estimators=50, max_depth=20, min_samples_leaf=2, min_samples_split=2, random_state=69)\n",
    "model_rfc2 = RandomForestClassifier(verbose=0, n_estimators=50, max_depth=20, min_samples_leaf=2, min_samples_split=2, random_state=69)\n",
    "model_rfc3 = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=69)\n",
    "# data = [(model_rfc1, X_train, X_test)]\n",
    "data = [(model_rfc3, X_train, X_test)]\n",
    "\n",
    "#data = [(model_rfc1, X_features_boot_train, X_features_boot_test)]\n",
    "#data = [(model_rfc1, X_train, X_test), (model_rfc2, X_train, X_test), (model_rfc3, X_train, X_test)]\n",
    "#data = [(model_rfc1, X_train, X_test), (model_rfc2, X_features_boot_train, X_features_boot_test)]\n",
    "\n",
    "for data_set in data:\n",
    "    model, X_train, X_test = data_set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Verifying model fit\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy} | F1 score: {f1} | {X_train.columns.values} | criterion: {model.criterion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calc_FNR_accuracy(y_true, y_pred):\n",
    "  conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "  for label_class in range(4):\n",
    "    \n",
    "    FN = sum(conf_matrix[label_class][i] for i in range(len(conf_matrix)) if i != label_class)  \n",
    "    \n",
    "    TP = conf_matrix[label_class][label_class]  \n",
    "    \n",
    "    TN = np.sum(np.delete(np.delete(conf_matrix, label_class, axis=0), label_class, axis=1))\n",
    "    \n",
    "    accuracy = (TP + TN) / np.sum(conf_matrix)\n",
    "    print(\"Accuracy for class\", label_class, \":\", accuracy)\n",
    "\n",
    "    FNR = FN / (FN + TP) if (FN + TP) > 0 else -1\n",
    "    print(\"FNR for class\", label_class, \":\", FNR)\n",
    "\n",
    "calc_FNR_accuracy([2, 0, 2, 2, 0, 1, 3, 3], [0, 0, 2, 2, 0, 2, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Export the model to a file\n",
    "joblib.dump(model_rfc1, 'model_rfc1.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for data_set in data:\n",
    "  model, X_train, X_test = data_set\n",
    "  rf_model = model\n",
    "  importances = rf_model.feature_importances_\n",
    "  scaled_importances = importances / np.max(importances)\n",
    "  values = X_train.columns\n",
    "  scaled_importances.sort()\n",
    "\n",
    "  for i in range(len(importances)):\n",
    "    print(values[i], \":\" ,f\"{scaled_importances[i]:.6f}\")\n",
    "  print(\"=====================================\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(model_rfc1, X, y, n_repeats=10, random_state=42)\n",
    "importances = result.importances_mean\n",
    "\n",
    "for feature, importance in zip(X.columns, importances):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_importances = importances / np.max(importances)\n",
    "for feature, importance in zip(X.columns, scaled_importances):\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "for i in range(3):\n",
    "    tree = model_rfc1.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_urls = pd.read_csv(\"datasets/phishtank_phish_urls.csv\")\n",
    "malicious_urls = pd.read_csv(\"datasets/malicious_phish.csv\")['url']\n",
    "\n",
    "phish_urls_in_malicious = phish_urls[phish_urls.isin(malicious_urls)]\n",
    "\n",
    "print(phish_urls_in_malicious.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_url = pd.read_csv(\"datasets/benign_urls.csv\")\n",
    "\n",
    "benign_urls_in_malicious = benign_url[benign_url.isin(malicious_urls)]\n",
    "\n",
    "print(benign_urls_in_malicious.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_set = pd.read_csv(\"datasets\\phish_set_X2.csv\")\n",
    "phish_pred = model_rfc1.predict(phish_set)\n",
    "print(len(phish_pred))\n",
    "print(np.count_nonzero(phish_pred == 0)/len(phish_pred))\n",
    "print(np.count_nonzero(phish_pred == 1)/len(phish_pred))\n",
    "print(np.count_nonzero(phish_pred == 2)/len(phish_pred))\n",
    "print(np.count_nonzero(phish_pred == 3)/len(phish_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_set = pd.read_csv(\"datasets/benign_dataset_X2.csv\")\n",
    "benign_pred = model_rfc1.predict(benign_set)\n",
    "print(len(benign_pred))\n",
    "count = np.count_nonzero(benign_pred == 0)\n",
    "print(np.count_nonzero(benign_pred == 0)/len(benign_pred))\n",
    "print(np.count_nonzero(benign_pred == 1)/len(benign_pred))\n",
    "print(np.count_nonzero(benign_pred == 2)/len(benign_pred))\n",
    "print(np.count_nonzero(benign_pred == 3)/len(benign_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_phish = pd.read_csv(\"datasets/malicious_phish.csv\")\n",
    "# Filter the DataFrame based on the conditions\n",
    "filtered_rows = malicious_phish[(malicious_phish['url'].str.contains(\"https://\")) & (malicious_phish['type'] == \"phishing\")]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(len(filtered_rows))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
