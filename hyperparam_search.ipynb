{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for hyperparameter searching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = pd.read_csv(\"datasets/feature_updated_dataset_X.csv\")\n",
    "y = pd.read_csv(\"datasets/feature_updated_dataset_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_ravel = y.values.ravel()\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring=scoring, cv=5, refit=False)\n",
    "grid_search.fit(X, y_ravel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for scorer in scoring:\n",
    "  best_index = np.argmax(grid_search.cv_results_['mean_test_'+scorer])\n",
    "  best_score = grid_search.cv_results_['mean_test_'+scorer][best_index]\n",
    "  best_params = grid_search.cv_results_['params'][best_index]\n",
    "  print(f\"Best {scorer} score: {best_score}\")\n",
    "  print(f\"Best {scorer} params: {best_params}\")\n",
    "  \n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df.to_csv('datasets/grid_search_results_4_parameters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy params: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Rank test accuracy: 1, mean test accuracy: 0.9172791762842902\n",
      "Rank test precision: 22, mean test precision: 0.9357108413921456\n",
      "Rank test recall: 1, mean test recall: 0.9172791762842902\n",
      "Rank test f1: 4, mean test f1: 0.9205063647122816\n",
      "Rank test overall: 28\n",
      "\n",
      "\n",
      "Best precision params: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Rank test accuracy: 7, mean test accuracy: 0.915786518215192\n",
      "Rank test precision: 1, mean test precision: 0.9372386950191766\n",
      "Rank test recall: 7, mean test recall: 0.915786518215192\n",
      "Rank test f1: 6, mean test f1: 0.9201414244119188\n",
      "Rank test overall: 21\n",
      "\n",
      "\n",
      "Best recall params: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Rank test accuracy: 1, mean test accuracy: 0.9172791762842902\n",
      "Rank test precision: 22, mean test precision: 0.9357108413921456\n",
      "Rank test recall: 1, mean test recall: 0.9172791762842902\n",
      "Rank test f1: 4, mean test f1: 0.9205063647122816\n",
      "Rank test overall: 28\n",
      "\n",
      "\n",
      "Best f1 params: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Rank test accuracy: 3, mean test accuracy: 0.9171762837528468\n",
      "Rank test precision: 5, mean test precision: 0.9367507026762172\n",
      "Rank test recall: 3, mean test recall: 0.9171762837528468\n",
      "Rank test f1: 1, mean test f1: 0.9209203641593504\n",
      "Rank test overall: 12\n",
      "\n",
      "\n",
      "Best overall params: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Rank test accuracy: 3, mean test accuracy: 0.9171762837528468\n",
      "Rank test precision: 5, mean test precision: 0.9367507026762172\n",
      "Rank test recall: 3, mean test recall: 0.9171762837528468\n",
      "Rank test f1: 1, mean test f1: 0.9209203641593504\n",
      "Rank test overall: 12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_df = pd.read_csv('datasets/grid_search_results_4_parameters.csv')\n",
    "read_df['rank_test_overall'] = read_df['rank_test_accuracy'] + read_df['rank_test_precision'] + read_df['rank_test_recall'] + read_df['rank_test_f1']\n",
    "score_strs = ['accuracy', 'precision', 'recall', 'f1', 'overall']\n",
    "\n",
    "for scorer in score_strs:\n",
    "  sorted = read_df.sort_values(by=f'rank_test_{scorer}', ascending=True)\n",
    "  top = sorted.head(1)\n",
    "  \n",
    "  rank_test_accuracy, mean_test_accuracy = top[['rank_test_accuracy', 'mean_test_accuracy']].values[0]\n",
    "  rank_test_precision, mean_test_precision = top[['rank_test_precision', 'mean_test_precision']].values[0]\n",
    "  rank_test_recall, mean_test_recall = top[['rank_test_recall', 'mean_test_recall']].values[0]\n",
    "  rank_test_f1, mean_test_f1 = top[['rank_test_f1', 'mean_test_f1']].values[0]\n",
    "  rank_test_overall = top[['rank_test_overall']].values[0][0]\n",
    "  \n",
    "  print(f'Best {scorer} params: {top[\"params\"].values[0]}')\n",
    "  print(f'Rank test accuracy: {rank_test_accuracy:.0f}, mean test accuracy: {mean_test_accuracy}')\n",
    "  print(f'Rank test precision: {rank_test_precision:.0f}, mean test precision: {mean_test_precision}')\n",
    "  print(f'Rank test recall: {rank_test_recall:.0f}, mean test recall: {mean_test_recall}')\n",
    "  print(f'Rank test f1: {rank_test_f1:.0f}, mean test f1: {mean_test_f1}')\n",
    "  print(f'Rank test overall: {rank_test_overall:.0f}')\n",
    "  \n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
